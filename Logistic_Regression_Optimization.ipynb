{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947ac575",
   "metadata": {},
   "source": [
    "# Algorithmic Foundations: Logistic Regression & Vectorization\n",
    "\n",
    "## 1. Project Goal\n",
    "The objective of this notebook is to implement **Logistic Regression from scratch** to understand the mathematical optimization behind the algorithm. \n",
    "\n",
    "We will focus on:\n",
    "1. **Mathematical Implementation:** Coding the Sigmoid activation and Cost function manually.\n",
    "2. **Vectorization:** replacing `for-loops` with Linear Algebra (Matrix operations) using NumPy.\n",
    "3. **Performance Analysis:** Benchmarking the training speed difference between iterative and vectorized approaches.\n",
    "\n",
    "## 2. Mathematical Background\n",
    "The core of Logistic Regression is minimizing the Cost Function $J(w,b)$ using **Gradient Descent**:\n",
    "$$w = w - \\alpha \\frac{\\partial J}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68842827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated: 10000 examples with 500 features each.\n",
      "Shape of X: (10000, 500), Shape of y: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "# 1. Generate Synthetic Data\n",
    "np.random.seed(1)\n",
    "m = 10000  # Number of examples (Large enough to see speed difference)\n",
    "n = 500    # Number of features\n",
    "\n",
    "# Create random feature matrix X and target vector y\n",
    "X = np.random.rand(m, n)\n",
    "y = np.random.randint(0, 2, (m, 1)) # Binary target (0 or 1)\n",
    "\n",
    "# Initialize parameters (weights w and bias b)\n",
    "w_init = np.random.rand(n, 1)\n",
    "b_init = 0.5\n",
    "\n",
    "print(f\"Dataset generated: {m} examples with {n} features each.\")\n",
    "print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef63f82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of 0: 0.5\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Quick test\n",
    "print(f\"Sigmoid of 0: {sigmoid(0)}\") # Should be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e47c1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Version 1: Unoptimized Loop (Iterative) ---\n",
    "def compute_gradient_loop(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n, 1))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        # Prediction for single example i\n",
    "        z_wb = 0\n",
    "        for j in range(n): \n",
    "            z_wb += X[i, j] * w[j]\n",
    "        z_wb += b\n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        # Error calculation\n",
    "        err = f_wb - y[i]\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        dj_db += err\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * X[i, j]\n",
    "            \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "# --- Version 2: Optimized Vectorization (Matrix Multiplication) ---\n",
    "def compute_gradient_matrix(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # 1. Calculate predictions for ALL examples at once\n",
    "    z = np.dot(X, w) + b     # Matrix Product\n",
    "    f_wb = sigmoid(z)        # Vectorized Sigmoid\n",
    "    \n",
    "    # 2. Calculate Error\n",
    "    err = f_wb - y\n",
    "    \n",
    "    # 3. Calculate Gradients using Transpose\n",
    "    dj_dw = (1/m) * np.dot(X.T, err)\n",
    "    dj_db = (1/m) * np.sum(err)\n",
    "    \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75773ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Performance Benchmark ---\n",
      "\n",
      "Loop Version Time:      7.3291 seconds\n",
      "Vectorized Version Time: 0.0037 seconds\n",
      "\n",
      "Result: Vectorization is 1970.3x faster!\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Performance Benchmark ---\\n\")\n",
    "\n",
    "# Test 1: Loop Version\n",
    "start_time = time.time()\n",
    "dj_db_loop, dj_dw_loop = compute_gradient_loop(X, y, w_init, b_init)\n",
    "end_time = time.time()\n",
    "time_loop = end_time - start_time\n",
    "print(f\"Loop Version Time:      {time_loop:.4f} seconds\")\n",
    "\n",
    "# Test 2: Vectorized Version\n",
    "start_time = time.time()\n",
    "dj_db_vec, dj_dw_vec = compute_gradient_matrix(X, y, w_init, b_init)\n",
    "end_time = time.time()\n",
    "time_vec = end_time - start_time\n",
    "print(f\"Vectorized Version Time: {time_vec:.4f} seconds\")\n",
    "\n",
    "# Calculate Speedup\n",
    "speedup = time_loop / time_vec\n",
    "print(f\"\\nResult: Vectorization is {speedup:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0245e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model...\n",
      "Iteration 0: Parameters updating...\n",
      "Iteration 100: Parameters updating...\n",
      "Iteration 200: Parameters updating...\n",
      "Iteration 300: Parameters updating...\n",
      "Iteration 400: Parameters updating...\n",
      "Iteration 500: Parameters updating...\n",
      "Iteration 600: Parameters updating...\n",
      "Iteration 700: Parameters updating...\n",
      "Iteration 800: Parameters updating...\n",
      "Iteration 900: Parameters updating...\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Use the optimized matrix function\n",
    "        dj_db, dj_dw = compute_gradient_matrix(X, y, w, b)\n",
    "        \n",
    "        # Update parameters\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Record cost (optional implementation for brevity)\n",
    "        if i % 100 == 0:\n",
    "             print(f\"Iteration {i}: Parameters updating...\")\n",
    "             \n",
    "    return w, b\n",
    "\n",
    "# Run Training\n",
    "print(\"Training final model...\")\n",
    "w_final, b_final = gradient_descent(X, y, w_init, b_init, alpha=0.1, num_iters=1000)\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e5881",
   "metadata": {},
   "source": [
    "## 3. Conclusion & Key Takeaways\n",
    "\n",
    "1. **Computational Efficiency:** The vectorized implementation is orders of magnitude faster (often >500x) than the loop-based approach. This is crucial when working with large datasets (Big Data).\n",
    "2. **Scalability:** By using Linear Algebra (`np.dot`), we leverage modern CPU/GPU architecture, allowing us to train models on millions of rows in seconds.\n",
    "3. **Core Understanding:** Implementing the gradient update rule manually ensures a deep understanding of how neural networks learn via backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
